# Actionable-email-classification

# Dataset

The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

>Dataset source: https://www.kaggle.com/wcukierski/enron-email-dataset

Due to large size i have not uploaded the datafile to github.

## I have implemented two approaches:
 1. Rule based Model on untagged data.
 2. Train a Machine Learning model on tagged data.

# 1. Rule based approach

## 1.1 setup env
The requirement for setting up the environment is present in requirements1.txt
After installation of spacy. download the "en_core_web_sm" model.<br> https://spacy.io/usage. <br>
I am using jupyter notebooks as my ide.

> **File Pre_processing1.ipynb:** 
Loads the raw email data,preprocesses it and extracts sentences. Extracted sentences are saved in the *sentence_file.csv*

> **File Model_rule_based.ipynb:**
Using token matching and regular expression on pos tags to detect actionable item patterns. classification labels are saved in file "rule_gen_label.csv" 
label 1: actionable  , label 0: non actionable

## 1.2 pipeline
- Loading the csv data using pandas
- The data file has 2 columns 'file' and 'message'
- Email body/content has to extracted from 'message' column
- checked for duplicate rows, no duplicates found
- message column splited on first occurance of '\n\n' gives the body/content.
- extracted sentences using NLTK sentence tokenizer english model.
- saved the sentences as csv file named as "sentence_file.csv"

- literature research on linguistic rule writting suggested to read through the sentences manually and find generalization for a particular class.
- Took some phrases that are actionable. and did token matching.
- from the phrases extracted the part of speech structural pattern.
- used Spacy for part of speech tagging.
- used regex rules on pos tags to parse the sentences and detect actionable items.
- rule based model generated labels are saved in file "rule_gen_label.csv"

## 1.3 challenges
- Data extraction was a challenge as before observing that content can be extracted just by spliting on '\n\n'. i was writing some complex regex.
- To get the sentences just spliting on '.'(dot), wouldnt work as it would split sentences that use dot for other purposes, used NLTK model to get sentences.
- Though data is quite big 6627375 sentences. i do have sufficient RAM but the time it would iterate all the sentences would be too high. hence classifying only 50,000
- Going though sentences to get the actionable phrases for token matching was not straight forward. few sentences felt ambiguous.
- finding those patterns in pos tagging was difficult. Had to go through some online resources. i have listed them in the jupyter notebooks


# 2. ML Model
> a) At first i have done one class classification/anamoly detection using Encoder and also one class classification SVM.
> b) Binary classification using BERT. 

## ML Part a

## a.1 setup env for part 'a'
The requirement for setting up the environment is present in requirements2.txt

> dataset: actions.csv: contains only action class and not non action class, using only one class done anamoly detection
> **File  Model_ML.ipynb:**

## ML Part b

## b.1 setup env for part 'b'
The requirement for setting up the environment is present in requirements3.txt

> dataset: created_data.csv: contains all the actions.csv class 1 tagged data plus randomly sampled class 0 data from the rule based approach
> **File creating_data.csv**
> **File Model_DL.ipynb**

> At least 12 GB of RAM and a GPU with more than 8 gb of memory will be required
> Run it on linux machine.
> The code in Model_DL.ipynb is not compatiple to be run on a windows system as the torch tensors are automatically set to 32 bit while using windows machine. 
> The code expects 64 bit torch tensors
> i will suggest a cloud machine or google colab to run it. 


## b.2 pipeline

- Loading the csv data using pandas
- using BERT english uncased for feature extraction. 
- The jupyter notebook has a detailed explanation of feature extraction using BERT
- 

