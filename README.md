# Actionable-email-classification

# Dataset

The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

>Dataset source: https://www.kaggle.com/wcukierski/enron-email-dataset

Due to large size i have not uploaded the datafile to github.

## I have implemented two approaches:
 1. Rule based Model on untagged data.
 2. Train a Machine Learning model on tagged data.

# 1. Rule based approach

## 1.1 setup env
The requirement for setting up the environment is present in 
<br>
>for windows use: requirements1.txt<br>
>for Linux use: req1_lin.txt
<br>
After installation of spacy. download the "en_core_web_sm" model.<br> https://spacy.io/usage. <br>
I am using jupyter notebooks as my ide.

> **File Pre_processing1.ipynb:** 
Loads the raw email data,preprocesses it and extracts sentences. Extracted sentences are saved in the *sentence_file.csv*

>**File Model_rule_based.ipynb:**
Using token matching and regular expression on pos tags to detect actionable item patterns. classification labels are saved in file "rule_gen_label.csv" 
label 1: actionable  , label 0: non actionable

## 1.2 pipeline
- Loading the csv data using pandas
- The data file has 2 columns 'file' and 'message'
- Email body/content has to extracted from 'message' column
- checked for duplicate rows, no duplicates found
- message column splited on first occurance of '\n\n' gives the body/content.
- extracted sentences using NLTK sentence tokenizer english model.
- saved the sentences as csv file named as "sentence_file.csv"

- literature research on linguistic rule writting suggested to read through the sentences manually and find generalization for a particular class.
- Took some phrases that are actionable. and did token matching.
- from the phrases extracted the part of speech structural pattern.
- used Spacy for part of speech tagging.
- used regex rules on pos tags to parse the sentences and detect actionable items.
- rule based model generated labels are saved in file "rule_gen_label.csv"

## 1.3 challenges
- Data extraction was a challenge as before observing that content can be extracted just by spliting on '\n\n'. i was writing some complex regex.
- To get the sentences just spliting on '.'(dot), wouldnt work as it would split sentences that use dot for other purposes, used NLTK model to get sentences.
- Though data is quite big 6627375 sentences. i do have sufficient RAM but the time it would iterate all the sentences would be too high. hence classifying only 50,000
- Going though sentences to get the actionable phrases for token matching was not straight forward. few sentences felt ambiguous.
- finding those patterns in pos tagging was difficult. Had to go through some online resources. i have listed them in the jupyter notebooks


# 2. ML Model
> a) At first i have done one class classification/anamoly detection using Encoder and also one class classification SVM.<br>
>b) Binary classification using BERT. 

## ML Part a

## a.1 setup env for part 'a'
The requirement for setting up the environment is present in <br>

>for windows use:requirements2.txt

<br>

>dataset: actions.csv: contains only action class and not non action class, using only one class done anamoly detection
<br>

>**File  Model_ anomaly_detection.ipynb:**

## ML Part b

## b.1 setup env for part 'b'
The requirement for setting up the environment is present in 
<br>

>for windows use: requirements3.txt<br>

>for linux use: req3_lin.txt

>dataset: created_data.csv: contains all the actions.csv class 1 tagged data plus randomly sampled class 0 data from the rule based approach
<br>

>**File creating_data.csv**
<br>

>**File Model_DL.ipynb**

> At least 12 GB of RAM and a GPU with more than 8 gb of memory will be required
> Run it on linux machine.
> The code in Model_DL.ipynb is not compatiple to be run on a windows system as the torch tensors are automatically set to 32 bit while using windows machine. 
> The code expects 64 bit torch tensors
> i will suggest a cloud machine or google colab to run it. 


## b.2 pipeline
- generate data file created_data.csv by running creating_data.ipynb
- Loading the csv data using pandas
- using BERT english uncased for feature extraction. 
- BERT is a method of pretraining language models.
- I will use BERT to extract high quality language features.
- I will use pre trained BERT model but will fine tune it on our data.
- Pre trained BERT model has a lot of information already encoded in its weights.
- I will lightly tune them to use the features for classification.

- In the past, I have Pretrained BERT on Hindi language from scratch.
- But for english so much work has already been done.I can never match the amount and quality of data, and the resources used by big research labs to pre train BERT.
- I will use a pre BERT trained Model, tune it on my data and extract features by transfer learning. 
- BERT is bidirectional it learns both left and right context.

### b.3 data preprocessing
- Apart from the other preprocessing
- BERT expects the inputs to be in certain format:
  - Add special tokens to the start and end of each sentence.
  - Pad & truncate all sentences to a single constant length(512)
  - Explicitly differentiate real tokens from padding tokens with the "attention mask".

- The trained model is saved in Model_Save folder

## b.4 challenges

- setting up required package for BERT,as Bert is originally written in python 2 and tensorflow 1
- Converting the data to the BERT input format.
- The memory usage of BERT is Very high even with batch size as small as 32.
- I didn't know at 1st that the torch.tensors are by default 32 bits floating point in windows machine.
- Got GPU memory error (8gb Vram) even with a batch size of 32, had to reduce it to 16.

## b.5 performance

- Recall,f1-score, and precision is almost maximum on trainning data.
- I have included the code to test performance on any dataset, by just changing the test file
- set TEST_FILE variable to the file name.
