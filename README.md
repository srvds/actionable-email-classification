# Actionable-email-classification

# Dataset

The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

>Dataset source: https://www.kaggle.com/wcukierski/enron-email-dataset

Due to large size i have not uploaded the datafile to github.

## I have implemented two approaches:
 1. Rule based Model on untagged data.
 2. Train a Machine Learning model on tagged data.

# 1. Rule based approach

## 1.1 setup env
The requirement for setting up the environment is present in requirements1.txt
After installation of spacy. download the "en_core_web_sm" model.<br> https://spacy.io/usage. <br>
I am using jupyter notebooks as my ide.

> **File Pre_processing1.ipynb:** 
Loads the raw email data,preprocesses it and extracts sentences. Extracted sentences are saved in the *sentence_file.csv*

> **File Model_ML.ipynb:**
Using token matching and regular expression on pos tags to detect actionable item patterns. classification labels are saved in file "rule_gen_label.csv" 
label 1: actionable  , label 0: non actionable

## 1.2 pipeline
- Loading the csv data using pandas
- The data file has 2 columns 'file' and 'message'
- Email body/content has to extracted from 'message' column
- checked for duplicate rows, no duplicates found
- message column splited on first occurance of '\n\n' gives the body/content.
- extracted sentences using NLTK sentence tokenizer english model.
- saved the sentences as csv file named as "sentence_file.csv"

- literature research on linguistic rule writting suggested to read through the sentences manually and find generalization for a particular class.
- Took some phrases that are actionable. and did token matching.
- from the phrases extracted the part of speech structural pattern.
- used Spacy for part of speech tagging.
- used regex rules on pos tags to parse the sentences and detect actionable items.
- rule based model generated labels are saved in file "rule_gen_label.csv"

## 1.3 challenges
- Data extraction was a challenge as before observing that content can be extracted just by spliting on '\n\n'. i was writing some complex regex.
- To get the sentences just spliting on '.'(dot), wouldnt work as it would split sentences that use dot for other purposes, used NLTK model to get sentences.
- Though data is quite big 6627375 sentences. i do have sufficient RAM but the time it would iterate all the sentences would be too high. hence classifying only 500,000
- Going though sentences to get the actionable phrases for token matching was not straight forward. few sentences felt ambiguous.
- finding those patterns in pos tagging was difficult. Had to go through some online resources. i have listed them in the jupyter notebooks


# 2. ML Model
